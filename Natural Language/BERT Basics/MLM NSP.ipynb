{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "!<img src=\"./images/MLM.png\" alt=\"MLM\" width=1024>",
   "id": "6ca8f819ad30af01"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-22T07:58:03.070997Z",
     "start_time": "2024-07-22T07:58:03.050578Z"
    }
   },
   "source": "from transformers import BertForMaskedLM, pipeline",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T07:58:06.007005Z",
     "start_time": "2024-07-22T07:58:03.129780Z"
    }
   },
   "cell_type": "code",
   "source": "bert_lm = BertForMaskedLM.from_pretrained('bert-base-cased')",
   "id": "66d3059ecb2cc936",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T07:58:06.021710Z",
     "start_time": "2024-07-22T07:58:06.007005Z"
    }
   },
   "cell_type": "code",
   "source": "bert_lm     # The last layer is a decoder 768 -> vocab_size",
   "id": "b055331bddc3b718",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForMaskedLM(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyMLMHead(\n",
       "    (predictions): BertLMPredictionHead(\n",
       "      (transform): BertPredictionHeadTransform(\n",
       "        (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (transform_act_fn): GELUActivation()\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "      (decoder): Linear(in_features=768, out_features=28996, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T07:58:06.245729Z",
     "start_time": "2024-07-22T07:58:06.021710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "cased_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "nlp = pipeline('fill-mask', model=bert_lm, tokenizer=cased_tokenizer)\n",
    "# or just nlp = pipeline('fill-mask', model='bert-base-cased')"
   ],
   "id": "accd3a131e1794c4",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T07:58:06.256832Z",
     "start_time": "2024-07-22T07:58:06.245729Z"
    }
   },
   "cell_type": "code",
   "source": "type(nlp.model)",
   "id": "1c9f074d2d0e7507",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.modeling_bert.BertForMaskedLM"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T07:58:06.433217Z",
     "start_time": "2024-07-22T07:58:06.256832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "preds = nlp(f\"if you don't {nlp.tokenizer.mask_token} at the sign, you will get a ticket.\")\n",
    "\n",
    "print(\"if you don't *** at the sign you will get a ticket.\")\n",
    "\n",
    "for p in preds:\n",
    "    print(f\"Token:{p['token_str']}. Score: {100*p['score']:,.2f}%\")\n",
    "\n",
    "p.keys()"
   ],
   "id": "2f21bc170f2ba6d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if you don't *** at the sign you will get a ticket.\n",
      "Token:stop. Score: 49.02%\n",
      "Token:look. Score: 40.27%\n",
      "Token:glance. Score: 1.28%\n",
      "Token:arrive. Score: 1.10%\n",
      "Token:turn. Score: 0.83%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['score', 'token', 'token_str', 'sequence'])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "!<img src=\"./images/NSP.png\" alt=\"SNP\" width=1024>",
   "id": "c129501b6ee00fcc"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T07:58:06.442715Z",
     "start_time": "2024-07-22T07:58:06.433217Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BertForNextSentencePrediction, BertTokenizer\n",
    "import torch"
   ],
   "id": "d099e66304e0df13",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T07:58:45.402909Z",
     "start_time": "2024-07-22T07:58:43.399046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "uncased_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "bert_nsp = BertForNextSentencePrediction.from_pretrained('bert-base-uncased')"
   ],
   "id": "da1135247044579",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForNextSentencePrediction: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForNextSentencePrediction from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T07:58:58.489395Z",
     "start_time": "2024-07-22T07:58:58.479609Z"
    }
   },
   "cell_type": "code",
   "source": "bert_nsp",
   "id": "d707d6c16bee485f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForNextSentencePrediction(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (cls): BertOnlyNSPHead(\n",
       "    (seq_relationship): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Labels meaning:\n",
    "- 0 == \"isNextSentence\"\n",
    "- 1 == \"notNextSentence\""
   ],
   "id": "8909bc12ef05e617"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T08:10:54.001220Z",
     "start_time": "2024-07-22T08:10:53.747306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the sentence pairs\n",
    "sentence_pairs = [\n",
    "    (\"The weather was perfect for a hike in the mountains.\", \"She packed her backpack and set out early in the morning.\"),\n",
    "    (\"The weather was perfect for a hike in the mountains.\", \"The city was bustling with activity as the festival began.\")\n",
    "]\n",
    "\n",
    "# Function to encode sentence pairs and make predictions\n",
    "def predict_nsp(model, tokenizer, sentence_pair):\n",
    "    encoding = tokenizer.encode_plus(*sentence_pair, return_tensors='pt')\n",
    "    input_ids = encoding['input_ids']\n",
    "    token_type_ids = encoding['token_type_ids']\n",
    "    attention_mask = encoding['attention_mask']\n",
    "\n",
    "    # Get the prediction logits\n",
    "    outputs = model(**encoding, labels=torch.LongTensor([1]))\n",
    "    # outputs = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask)\n",
    "    logits = outputs.logits\n",
    "    softmax = torch.nn.Softmax(dim=1)\n",
    "    probabilities = softmax(logits)\n",
    "    predicted_label = torch.argmax(probabilities).item()\n",
    "    \n",
    "    return predicted_label, probabilities, logits, outputs\n",
    "\n",
    "# Predict and print results for each sentence pair\n",
    "for idx, pair in enumerate(sentence_pairs):\n",
    "    label, probs, logits, outputs = predict_nsp(bert_nsp, uncased_tokenizer, pair)\n",
    "    print(f\"Sentence Pair {idx + 1}:\")\n",
    "    print(f\"Sentence A: {pair[0]}\")\n",
    "    print(f\"Sentence B: {pair[1]}\")\n",
    "    print(f\"Prediction: {'Next Sentence' if label == 0 else 'Not the Next Sentence'}\")\n",
    "    print(f\"Logits: {logits.tolist()}\")\n",
    "    print(f\"Probabilities: {probs.tolist()}\")\n",
    "    print(f\"Outputs: {outputs}\\n\")\n"
   ],
   "id": "ac834b28e3bd372c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence Pair 1:\n",
      "Sentence A: The weather was perfect for a hike in the mountains.\n",
      "Sentence B: She packed her backpack and set out early in the morning.\n",
      "Prediction: Next Sentence\n",
      "Logits: [[5.675405502319336, -5.149261474609375]]\n",
      "Probabilities: [[0.999980092048645, 1.99020687432494e-05]]\n",
      "Outputs: NextSentencePredictorOutput(loss=tensor(10.8247, grad_fn=<NllLossBackward0>), logits=tensor([[ 5.6754, -5.1493]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "\n",
      "Sentence Pair 2:\n",
      "Sentence A: The weather was perfect for a hike in the mountains.\n",
      "Sentence B: The city was bustling with activity as the festival began.\n",
      "Prediction: Next Sentence\n",
      "Logits: [[4.564054012298584, -3.68455171585083]]\n",
      "Probabilities: [[0.9997383952140808, 0.00026155461091548204]]\n",
      "Outputs: NextSentencePredictorOutput(loss=tensor(8.2489, grad_fn=<NllLossBackward0>), logits=tensor([[ 4.5641, -3.6846]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)\n",
      "\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Common Fine Tuned Berts",
   "id": "2d17e0d1e3007b7d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T08:21:47.122020Z",
     "start_time": "2024-07-22T08:21:47.108127Z"
    }
   },
   "cell_type": "code",
   "source": "from transformers import pipeline, BertForSequenceClassification, BertForTokenClassification, BertForQuestionAnswering",
   "id": "6a9c4d42601753d6",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "!<img src=\"./images/SeqClass.png\" alt=\"Sequence Classification\" width=1024>\n",
    "Same as NSP just with single sequence"
   ],
   "id": "9d28b8ba5804b42c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T08:22:56.954574Z",
     "start_time": "2024-07-22T08:22:54.447055Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bert_sq = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "bert_sq, bert_sq.classifier"
   ],
   "id": "537bad525db70c84",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(BertForSequenceClassification(\n",
       "   (bert): BertModel(\n",
       "     (embeddings): BertEmbeddings(\n",
       "       (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "       (position_embeddings): Embedding(512, 768)\n",
       "       (token_type_embeddings): Embedding(2, 768)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (encoder): BertEncoder(\n",
       "       (layer): ModuleList(\n",
       "         (0-11): 12 x BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (intermediate_act_fn): GELUActivation()\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "     (pooler): BertPooler(\n",
       "       (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "       (activation): Tanh()\n",
       "     )\n",
       "   )\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       " ),\n",
       " Linear(in_features=768, out_features=3, bias=True))"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "!<img src=\"./images/TknClass.png\" alt=\"Token Classification\" width=1024>\n",
   "id": "4f0658ecf7571c0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T08:25:04.420155Z",
     "start_time": "2024-07-22T08:25:03.076225Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bert_tc = BertForTokenClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
    "bert_tc, bert_tc.classifier"
   ],
   "id": "930e2c045ad55f85",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForTokenClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(BertForTokenClassification(\n",
       "   (bert): BertModel(\n",
       "     (embeddings): BertEmbeddings(\n",
       "       (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "       (position_embeddings): Embedding(512, 768)\n",
       "       (token_type_embeddings): Embedding(2, 768)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (encoder): BertEncoder(\n",
       "       (layer): ModuleList(\n",
       "         (0-11): 12 x BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (intermediate_act_fn): GELUActivation()\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (dropout): Dropout(p=0.1, inplace=False)\n",
       "   (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       " ),\n",
       " Linear(in_features=768, out_features=3, bias=True))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "!<img src=\"./images/QA.png\" alt=\"QA\" width=1024>",
   "id": "6a1fc721baad1eeb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-22T08:25:42.696299Z",
     "start_time": "2024-07-22T08:25:41.159438Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bert_qa = BertForQuestionAnswering.from_pretrained('bert-base-uncased')\n",
    "bert_qa, bert_qa.qa_outputs"
   ],
   "id": "e1e8fa6d7809f1b5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForQuestionAnswering: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(BertForQuestionAnswering(\n",
       "   (bert): BertModel(\n",
       "     (embeddings): BertEmbeddings(\n",
       "       (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "       (position_embeddings): Embedding(512, 768)\n",
       "       (token_type_embeddings): Embedding(2, 768)\n",
       "       (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "       (dropout): Dropout(p=0.1, inplace=False)\n",
       "     )\n",
       "     (encoder): BertEncoder(\n",
       "       (layer): ModuleList(\n",
       "         (0-11): 12 x BertLayer(\n",
       "           (attention): BertAttention(\n",
       "             (self): BertSelfAttention(\n",
       "               (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "             (output): BertSelfOutput(\n",
       "               (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "               (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "               (dropout): Dropout(p=0.1, inplace=False)\n",
       "             )\n",
       "           )\n",
       "           (intermediate): BertIntermediate(\n",
       "             (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "             (intermediate_act_fn): GELUActivation()\n",
       "           )\n",
       "           (output): BertOutput(\n",
       "             (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "             (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "             (dropout): Dropout(p=0.1, inplace=False)\n",
       "           )\n",
       "         )\n",
       "       )\n",
       "     )\n",
       "   )\n",
       "   (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       " ),\n",
       " Linear(in_features=768, out_features=2, bias=True))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "a58f35128ea3bef"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
